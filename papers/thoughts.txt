
Gradient Without Propagation:
- We could do a systematic review of the FGD algorithm on different architectures and types of neural networks
- Once we know which types benefit from FGD and which don't, we could also try a hybrid approach using both FGD on layers which are known to benefit from this algorithm
and standard Backprop on other layer where we might have bottlenecks
- One other thing we could do is exploring different sampling strategies other than from a normal distribution
(which is what is proposed by the paper)


WNGrad: Learn the Learning Rate
- We could study the effectiveness of the algorithm accross different kinds of ML models (Decision Trees, SVM,
Transformers)
- What about highly non-convex and complex losses (e.g Reinforcement Learning), can WNGrad provide any benefits?


Global Optimization with Parametric Function Approximation:
- One idea that comes to mind is testing cross-domain application of this method (test the
method on different models and datasets to see if it generalizes well)
- Try addapting the GO-UCB Algorithm for Reinforcement Learning and see if we can get faster convergence and or
improved performance.
